{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| :\u001b[43m \u001b[0m|\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym #Importa a biblioteca Gym, que é usada para criar e manipular ambientes de simulação de aprendizado por reforço.\n",
    "import random #Importa a biblioteca Random, que fornece funções para gerar números aleatórios.\n",
    "\n",
    "random.seed(1234) #Define a semente para o gerador de números aleatórios.\n",
    "\n",
    "# Cria uma instância do ambiente Taxi-v3 usando a biblioteca Gym\n",
    "# render_mode='ansi' é usado para renderizar o ambiente como texto que pode ser impresso no console\n",
    "streets = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "\n",
    "# Reinicializa o ambiente para o estado inicial, pronto para começar um novo episódio\n",
    "streets.reset()\n",
    "\n",
    "# Renderiza o ambiente como texto e imprime a representação do estado inicial no console\n",
    "print (\"\\n\" + streets.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'+---------+\\n|R: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| :\\x1b[43m \\x1b[0m|\\x1b[35mB\\x1b[0m: |\\n+---------+\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_state =  streets.encode(2, 3, 2, 0)\n",
    "\n",
    "streets.s = initial_state\n",
    "\n",
    "streets.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 368, -1, False)],\n",
       " 1: [(1.0, 168, -1, False)],\n",
       " 2: [(1.0, 288, -1, False)],\n",
       " 3: [(1.0, 248, -1, False)],\n",
       " 4: [(1.0, 268, -10, False)],\n",
       " 5: [(1.0, 268, -10, False)]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Acessa as transições possíveis no ambiente a partir do estado inicial \n",
    "# Isso retorna um dicionário onde as chaves são ações e os valores são listas de transições possíveis\n",
    "# Cada transição é uma tupla (probabilidade, próximo_estado, recompensa, feito)\n",
    "streets.P[initial_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "\n",
    "# Cria uma Q-table inicializada com zeros, com dimensões baseadas no número de estados e ações possíveis no ambiente\n",
    "q_table = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
    "\n",
    "# Define a taxa de aprendizado, o fator de desconto, a taxa de exploração e o número de épocas (episódios) de treinamento\n",
    "learning_rate = 0.1  # Taxa de aprendizado - define o quanto as novas informações substituem as antigas\n",
    "discount_factor = 0.6  # Fator de desconto - define a importância das recompensas futuras\n",
    "exploration = 0.1  # Taxa de exploração - define a probabilidade de escolher uma ação aleatória (explorar) versus a melhor ação conhecida (explorar)\n",
    "epochs = 10000  # Número de épocas - define quantas vezes o agente vai jogar o jogo para aprender\n",
    "\n",
    "# Loop principal para rodar múltiplos episódios de treinamento\n",
    "for taxi_run in range(epochs):\n",
    "    # Reinicializa o ambiente para o estado inicial e captura o estado inicial e informações adicionais\n",
    "    state, info = streets.reset()\n",
    "    done = False  # Inicializa a variável 'done' para verificar se o episódio terminou\n",
    "    \n",
    "    # Loop interno para cada passo dentro do episódio\n",
    "    while not done:\n",
    "        # Gera um valor aleatório entre 0 e 1 para decidir entre exploração e exploração\n",
    "        random_value = random.uniform(0, 1)\n",
    "        if random_value < exploration:\n",
    "            # Escolhe uma ação aleatória do espaço de ações do ambiente (exploração)\n",
    "            action = streets.action_space.sample()\n",
    "        else:\n",
    "            # Escolhe a melhor ação com base nos valores Q da Q-table para o estado atual (explorar)\n",
    "            action = np.argmax(q_table[state])\n",
    "\n",
    "        # Executa a ação escolhida e observa o próximo estado, a recompensa, se o episódio terminou (done), se foi truncado (truncated) e informações adicionais (info)\n",
    "        next_state, reward, done, truncated, info = streets.step(action)\n",
    "\n",
    "        # Armazena o valor Q atual para o par (estado, ação)\n",
    "        prev_q = q_table[state, action]\n",
    "        # Encontra o melhor valor Q para o próximo estado\n",
    "        next_max_q = np.max(q_table[next_state])\n",
    "        # Calcula o novo valor Q usando a fórmula de atualização do Q-learning\n",
    "        new_q = (1 - learning_rate) * prev_q + learning_rate * (reward + discount_factor * next_max_q)\n",
    "        # Atualiza a Q-table com o novo valor Q\n",
    "        q_table[state, action] = new_q\n",
    "\n",
    "        # Atualiza o estado atual para o próximo estado\n",
    "        state = next_state\n",
    "        # Verifica se o episódio terminou ou foi truncado e atualiza a variável 'done'\n",
    "        done = done or truncated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.41219737, -2.40876978, -2.39719389, -2.3639511 , -8.93846373,\n",
       "       -6.30556416])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Acessa a linha da tabela Q (q_table) que corresponde ao estado inicial (initial_state), retornando os valores Q para todas as ações possíveis neste estado.\n",
    "\n",
    "q_table[initial_state]\n",
    "\n",
    "# array([-2.40148903, -2.39484223, -2.39833537, -2.3639511 , -8.97576456, -8.04553863]) \n",
    "# -2.40148903: Valor Q para a ação 0 (Sul)\n",
    "# -2.39484223: Valor Q para a ação 1 (Norte)\n",
    "# -2.39833537: Valor Q para a ação 2 (Leste)\n",
    "# -2.3639511: Valor Q para a ação 3 (Oeste) \n",
    "# -8.97576456: Valor Q para a ação 4 (Pegar Passageiro)\n",
    "# -8.04553863: Valor Q para a ação 5 (Deixar Passageiro)\n",
    "# Valores negativos indicam penalidades ou custos associados às ações no estado inicial. Ações com valores menos negativos são consideradas melhores pelo algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trip number 10\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output  # Importa a função clear_output da biblioteca IPython para limpar a saída da célula\n",
    "from time import sleep  # Importa a função sleep da biblioteca time para pausar a execução do código\n",
    "\n",
    "# Loop principal que executa 10 viagens no ambiente Taxi-v3\n",
    "for tripnum in range(1, 11):\n",
    "    state, info = streets.reset()  # Reinicializa o ambiente e captura o estado inicial e informações adicionais\n",
    "    \n",
    "    # Verificação para garantir que `state` é um inteiro\n",
    "    if not isinstance(state, int):\n",
    "        raise ValueError(f\"O estado deve ser um inteiro, mas é {type(state)}: {state}\")\n",
    "    \n",
    "    done = False  # Inicializa a variável `done` como False para indicar que o episódio não terminou\n",
    "    \n",
    "    # Loop interno que continua até que `done` seja True, ou seja, até que o episódio termine\n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])  # Escolhe a ação com o maior valor Q no estado atual usando a Q-table\n",
    "        # Executa a ação escolhida no ambiente e captura o próximo estado, recompensa, se o episódio terminou,\n",
    "        # se foi truncado e informações adicionais\n",
    "        next_state, reward, done, truncated, info = streets.step(action)\n",
    "        \n",
    "        clear_output(wait=True)  # Limpa a saída da célula atual no notebook Jupyter\n",
    "        print(\"Trip number \" + str(tripnum))  # Imprime o número da viagem atual\n",
    "        print(streets.render())  # Renderiza o ambiente atual como texto e imprime no console\n",
    "        \n",
    "        sleep(0.5)  # Pausa a execução do código por 0.5 segundos para tornar a visualização do ambiente mais lenta\n",
    "        \n",
    "        state = next_state  # Atualiza o estado atual para o próximo estado\n",
    "    \n",
    "    sleep(2)  # Pausa a execução do código por 2 segundos antes de iniciar a próxima viagem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desafio!:\n",
    "\n",
    "Modifique o bloco acima para acompanhar o total de passos de tempo e use isso como uma métrica para avaliar a eficácia do nosso sistema de Q-learning. Você pode querer aumentar o número de viagens simuladas e remover as chamadas sleep() para permitir que o sistema seja executado com mais amostras.\n",
    "\n",
    "Agora, experimente ajustar os hiperparâmetros. Até que ponto o número de épocas pode ser reduzido antes que nosso modelo comece a sofrer? Você consegue encontrar melhores taxas de aprendizado, fatores de desconto ou fatores de exploração para tornar o treinamento mais eficiente? A taxa de exploração vs. exploração, em particular, é interessante para experimentar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
